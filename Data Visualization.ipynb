{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Absenteeism in Toms River High School East**\n![banner.jpg](https://www.trschools.com/hseast/imgs/banner.jpg)\n\n***\n\n## Introduction\nToms River High School East (TRSHE), a comprehensive four-year high school located in New Jersey, is affected by large absent rates. In fact, the chronic absenteeism rates in TRSHE has been above the NJ state average rates from grades 9th to 12th. This kernel's objective is to better understand the underlying patterns of TRSHE absenteeism, how certain factors contribute to these patterns, and the type of methods that are best for prediction.\n\nT-Test to check for significance.\n\nOutline:\n1. [Data Overview](#section-one)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\n\nstudent_data = pd.read_csv('../input/dataproj/High School East Student Data - Sheet1.csv')","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# 1. Data Overview\n\nThe data was collected through the help of Mrs. Anders and Dr. Kretz, both faculty of TRSHE. Certain variables were chosen because chronic absenteeism is susceptible to variables like **Limited English Proficiency, 504/IDEA Disability, Race/Ethnicity**, and likewise. A comprehensive and large dataset of students in the United States conducted by the National Center for Education Statistics exists, but many important variables were surpressed for public-use. Accessing that data is a **rigorous** and **security-tight** process involving various academic officers that no regular person could pass. So that wasn't happening. So I resorted to my school data.\n\nThus, the data collected for this project was **manually** recorded. \n***\n\n**A6-A12**: represents absences from 6th grade to 12th grade\n\n**T6-T12**: represents tardies from 6th grade to 12th grade\n\n**IEP/Specialized**: represents whether a student is in special education"},{"metadata":{"trusted":true},"cell_type":"code","source":"student_data.head()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"  Student English Langauge Learner Has a Disability?  \\\n0      CA                  Yes/No?                No   \n1      CI                       No                No   \n2     CIS                      NaN                No   \n3     DIP                      NaN                No   \n4      EA                      NaN                No   \n\n  Student on Free or Reduced Lunch     Race/Ethnic A6  A7 A8 A9 A10  ...  \\\n0                               No           Asian  1   0  0  0   1  ...   \n1                              NaN           White  5   9  5  6   9  ...   \n2                              NaN  White/Hispanic  0   0  0  0   0  ...   \n3                              NaN           White  2   7  8  7  10  ...   \n4                              NaN           White  7  10  7  4   1  ...   \n\n    A12  T6 T7 T8 T9 T10  T11  T12  Gender IEP/Specialized  \n0   0.0   9  4  5  1   2    2    1       M              No  \n1   3.0   2  2  3  4   2    4    1       F              No  \n2   2.0   0  0  0  0   0    5    2       F              No  \n3  13.0   1  0  3  6   7    9    3       F              No  \n4   1.0   1  0  0  0   5    0    0       F              No  \n\n[5 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Student</th>\n      <th>English Langauge Learner</th>\n      <th>Has a Disability?</th>\n      <th>Student on Free or Reduced Lunch</th>\n      <th>Race/Ethnic</th>\n      <th>A6</th>\n      <th>A7</th>\n      <th>A8</th>\n      <th>A9</th>\n      <th>A10</th>\n      <th>...</th>\n      <th>A12</th>\n      <th>T6</th>\n      <th>T7</th>\n      <th>T8</th>\n      <th>T9</th>\n      <th>T10</th>\n      <th>T11</th>\n      <th>T12</th>\n      <th>Gender</th>\n      <th>IEP/Specialized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CA</td>\n      <td>Yes/No?</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Asian</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>M</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CI</td>\n      <td>No</td>\n      <td>No</td>\n      <td>NaN</td>\n      <td>White</td>\n      <td>5</td>\n      <td>9</td>\n      <td>5</td>\n      <td>6</td>\n      <td>9</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>2</td>\n      <td>4</td>\n      <td>1</td>\n      <td>F</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CIS</td>\n      <td>NaN</td>\n      <td>No</td>\n      <td>NaN</td>\n      <td>White/Hispanic</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2</td>\n      <td>F</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DIP</td>\n      <td>NaN</td>\n      <td>No</td>\n      <td>NaN</td>\n      <td>White</td>\n      <td>2</td>\n      <td>7</td>\n      <td>8</td>\n      <td>7</td>\n      <td>10</td>\n      <td>...</td>\n      <td>13.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>6</td>\n      <td>7</td>\n      <td>9</td>\n      <td>3</td>\n      <td>F</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>EA</td>\n      <td>NaN</td>\n      <td>No</td>\n      <td>NaN</td>\n      <td>White</td>\n      <td>7</td>\n      <td>10</td>\n      <td>7</td>\n      <td>4</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>F</td>\n      <td>No</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 21 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(student_data.iloc[0].values[5:12]) # absence columns\nprint(student_data.iloc[0].values[12:19]) #tardy columns","execution_count":15,"outputs":[{"output_type":"stream","text":"['1' '0' '0' '0' '1' 2.0 0.0]\n['9' '4' '5' '1' 2 2 1]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Because some absent and tardy count columns contains a string (\"TRANSFER\"), some of the variables in the **A6** to **A12** and **T6** to **T12** columns are represented as strings instead of integers. So I made some adjustments and converted the necessary values to int values. \n\nhttps://stackoverflow.com/questions/59084770/one-hot-encoder-what-is-the-industry-norm-to-encode-before-train-split-or-after\n\n## 1.1 Preparing Data for Graph\n\nI make a separate instance of student_data because I will be preproccessing data before the train test split in order for searborn and matplotlib to work. Here, I convert number values that are strings into an int type even if the string indicates a float type (some students have absences like 2.5 for a given year). Then, I convert \"TRANSFER\" to equal 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataForGraph = pd.read_csv('../input/dataproj/High School East Student Data - Sheet1.csv')\n\n#easy way of accessing A_6, A_7, ... A_N columns\ndef column_list(letter, start, end):\n    return [\"%s%d\" % (letter, i) for i in range(start, end)]\n\n#convert strings to int type even if it's a float\ndef convertStat(x):\n    \n    if(isinstance(x, int) == False):\n        \n        #we don't know if the string is float or int\n        #converting it to int if it's not an int will cause an error\n        try:\n            return 0 if x == \"TRANSFER\" else int(x)\n        except:\n            pass\n        \n        #if it can't pass as an int, then it must be a float that will be converted to an int\n        #the float is rounded\n        return 0 if x == \"TRANSFER\" else int(float(x))\n    else:\n        return x\n    \nfor i in [\"A\", \"T\"]:\n    for j in column_list(i, 6, 13):\n        dataForGraph[j] = dataForGraph[j].apply(convertStat)\n\nprint(list(dataForGraph.iloc[0].values[5:12])) # absence columns of first student\nprint(list(dataForGraph.iloc[0].values[12:19])) #tardy columns\n","execution_count":34,"outputs":[{"output_type":"stream","text":"[1, 0, 0, 0, 1, 2, 0]\n[9, 4, 5, 1, 2, 2, 1]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#sums all absences and tardies from all grades\nstudent_data['AbsentSum'] = student_data[column_list('A', 6, 13)].sum(axis=1)\nstudent_data['TardySum'] = student_data[column_list('T', 6, 13)].sum(axis=1)\n\n#sum absences in middle and high school\nstudent_data['AbsencesSum_MS'] = student_data[column_list('A', 6, 9)].sum(axis=1)\nstudent_data['AbsencesSum_HS'] = student_data[column_list('A', 9, 13)].sum(axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Train/Test Split and Preproccessing\n\nConverting needed values into usable int types for learning models I also sum the absences and tardies into a new column. All of this happens after the test split.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_seed = 1\ntest_size = 0.2\n\nfeatures = [\"A6\", \"A7\", \"A8\", \"A9\", \"T6\", \"T7\", \"T8\", \"Gender\", \"IEP/Specialized\"]\ny_objective = student_data[\"AbsencesSum_HS\"]\n\n#split and preproccess\ntrain_X, val_X, train_y, val_y = train_test_split(student_data[features], y_objective, random_state=random_seed, test_size=test_size)\n\ntrain_X = pd.get_dummies(train_X)\nval_X = pd.get_dummies(val_X)\n\n# get_dummies creates different columns,but each set needs to have equal # of features\nmissing_cols = set( train_X.columns ) - set( val_X.columns )\nfor c in missing_cols:\n    val_X[c] = 0\n\nval_X = val_X[train_X.columns]","execution_count":4,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'AbsencesSum_HS'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'AbsencesSum_HS'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-adc61dc272ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"A6\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"A7\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"A8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"A9\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"T6\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"T7\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"T8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Gender\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IEP/Specialized\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my_objective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"AbsencesSum_HS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#split and preproccess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'AbsencesSum_HS'"]}]},{"metadata":{},"cell_type":"markdown","source":"### To address the issue of \"TRANSFER\" values converting tardy and absent columns into strings:"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n###### CHANGE. PREPROCCESS THE DATA AFTER THE TRAIN TEST SPLIT. BUT FOR NOW, KEEP IT THE WAY SO YOU CAN PLOT\n\n#convert strings to int type even if it's a float\ndef convertStat(x):\n    \n    if(isinstance(x, int) == False):\n        \n        #we don't know if the string is float or int\n        #converting it to int if it's not an int will cause an error\n        try:\n            return 0 if x == \"TRANSFER\" else int(x)\n        except:\n            pass\n        \n        #if it can't pass as an int, then it must be a float that will be converted to an int\n        return 0 if x == \"TRANSFER\" else int(float(x))\n    else:\n        return x\n    \nfor convertColumnData(df):\n    for i in range(6,13):\n        absences_column = \"A%d\" % i\n        tardies_column = \"T%d\" % i\n\n        df[absences_column] = df[absences_column].apply(convertStat)\n        df[tardies_column] = df[tardies_column].apply(convertStat)\n        \ncovertColumnData(train_X)\ncovertColumnData(val_X)\n    \n    \n#convert absenses and tardies to integers for all students\n\n\nprint(list(student_data.iloc[0].values[5:12])) # absence columns of first student\nprint(list(student_data.iloc[0].values[12:19])) #tardy columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Evaluating Data and Preproccessing\n\nPlotting to see if there are any glaryingly obvious patterns or relationships between variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,7))\n#plt.figure(figsize=(2,10))\n\n# 6:13 represents the absent columns\n# 13:20 represents the tardy columns\nx_values = range(6,13)\nfor i in range(len(student_data)):\n    absences_y = student_data.iloc[i].values[5:12]\n    tardies_y = student_data.iloc[i].values[12:19]\n\n    ax1.plot(x_values, np.array(absences_y), alpha=0.7)    \n    ax2.plot(x_values, np.array(tardies_y), alpha=0.7)\n\nax1.set_title(\"Absences from 6th to 12th grade\")\nax2.set_title(\"Tardies from 6th to 12th grade\")\n\nax1.set_ylabel(\"Absences\")\nax2.set_ylabel(\"Tardies\")\n\nax2.set_xlabel(\"Grade\")\nax1.set_xlabel(\"Grade\")\n\nplt.subplots_adjust(wspace=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, there isn't a one-fit-all absent and tardy pattern across all the grade levels. There isn't much of a clear pattern and this is due to the multitude of factors that contribute to absences throughout the years. But there seems to be similarity in the peaks, so it would be useful to see the correlation between absences and tardies. [Based on TRSHE data,](https://rc.doe.state.nj.us/report.aspx?type=school&lang=english&county=29&district=5190&school=030&schoolyear=2018-2019#P99cba7ec593f446e8cbf8d62c3db0208_2_oHit0) the chronic absent rates grow throughout the grade levels. At grade 12, **21**% of students, 2% higher than the NJ state average, have been chronically absent. In grade 11, **20**% of TRSHE students were chronically absent, which is 6% higher than the NJ state average.\n\nFor our dataset, **what is the distrubtion of tardies/absences over the grades, and what are some variable relationships?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.clf()\n\n#easy way of accessing A_6, A_7, ... A_N columns\ndef column_list(letter, start, end):\n    return [\"%s%d\" % (letter, i) for i in range(start, end)]\n        \n#sums all absences and tardies from all grades\nstudent_data['AbsentSum'] = student_data[column_list('A', 6, 13)].sum(axis=1)\nstudent_data['TardySum'] = student_data[column_list('T', 6, 13)].sum(axis=1)\n\n#sum absences in middle and high school\nstudent_data['AbsencesSum_MS'] = student_data[column_list('A', 6, 9)].sum(axis=1)\nstudent_data['AbsencesSum_HS'] = student_data[column_list('A', 9, 13)].sum(axis=1)\n\n\n# ============================= ABSENCES AND TARDIES PLOTTING =====================================\n\n# middle school tardies vs middle school absences\nfg, (ax1, ax2) = plt.subplots(1,2, figsize=(12,6))\n\n#First plot\nax1.title.set_position([.5, 1.05])\n#sns.regplot(x=\"TardySum\", y=\"AbsentSum\", data=student_data, ax=ax1)\nsns.scatterplot(x=\"TardySum\", y=\"AbsentSum\", hue=\"Student\", data=student_data, legend=False, ax=ax1)\nax1.set(xlabel=\"Sum of Tardies\", ylabel=\"Sum of Absences\", title=\"Relationship between Tardies and Sums\")\n\n#Second plot\nax2.title.set_position([.5, 1.05])\n#sns.regplot(x=\"AbsencesSum_MS\", y=\"AbsencesSum_HS\", data=student_data, ax=ax2)\nsns.scatterplot(x=\"AbsencesSum_MS\", y=\"AbsencesSum_HS\", hue=\"Student\", data=student_data, legend=False, ax=ax2)\nax2.set(xlabel=\"High School Absences\", ylabel=\"Middle School Absences\", title=\"Relationship between High School and Middle School Absences\")\n\n\nplt.subplots_adjust(wspace=0.5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, there is a linear relathionship between absences and tardies. On average, the number of tardies is less than the number of absences."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.clf()\n\n#sns.distplot(student_data[\"AbsencesSum_HS\"], bins=5, kde=False)\nHistbins = range(0,100,10)\n\n#plt.hist(student_data[\"AbsencesSum_HS\"], bins=Histbins, edgecolor=\"black\")\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,6))\n\nsns.distplot(student_data[\"AbsencesSum_HS\"], bins=Histbins, kde=False, rug=True, ax=ax1)\nsns.boxplot(x=\"AbsencesSum_HS\", data=student_data, ax=ax2)\n\n\n#plt.xticks(np.arange(0, 95, 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Learning Models\n\n## 3.1 Decision Trees/Random Forests\n\nObjective is to find the optimal number of leaves for each decision tree. The next step is to compare each model to each other for the most optimal model to use."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\n\n#==================== Validate the Decision Tree Model ===========================\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    \n    return mae\n\nleaf_node_range = range(2,30)\nscores = {leaf_size: get_mae(leaf_size, train_X, val_X, train_y, val_y) for leaf_size in leaf_node_range}\nlow = min(scores, key=scores.get)\nstudent_tree_model = DecisionTreeRegressor(random_state=1, max_leaf_nodes=min(scores, key=scores.get))\nstudent_tree_model.fit(train_X, train_y)\n\nabsences_predictions = student_tree_model.predict(val_X)\nmae = mean_absolute_error(absences_predictions, val_y)\n\nprint(\"Validation MAE: {:,.0f}\".format(mae))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://towardsdatascience.com/why-random-forests-outperform-decision-trees-1b0f175a0b5\nhttps://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n\nhttps://towardsdatascience.com/optimizing-hyperparameters-in-random-forest-classification-ec7741f9d3f6"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\n\nrandom_forest = RandomForestRegressor(random_state=random_seed,\n                                    max_features=\"log2\",\n                                    n_estimators=400,\n                                    max_leaf_nodes=low,\n                                    min_samples_split=12,\n                                    min_samples_leaf=40)\n\nmae = []\nrandom_forest.fit(train_X, train_y)\nabsences_predictions = random_forest.predict(val_X)\nprint(mean_absolute_error(absences_predictions, val_y))\n\nscores = -1 * cross_val_score(student_tree_model, train_X, train_y,\n                              cv=5,\n                            scoring='neg_mean_absolute_error')\n\nprint(scores.mean())\nprint(list(scores))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nmodels = []\nmodels.append(('LogisticRegression', LogisticRegression()))\n\ndef runModels(models, trainX, trainY, valY, cv):\n    cross_valid = []\n    for i in models:\n        model = i[1]\n        scores = -1 * cross_val_score(model, \n            trainX, \n            trainY,\n            cv=cv,\n            scoring=\"mean_absolute_error\")\n\n        cross_valid.append((i[0], scores.mean(), scores))\n\n    return cross_valid\n\nlogReg = LogisticRegression(random_state=1, max_iter=10000)\nlogReg.fit(train_X, train_y)\nabsences_predictions = logReg.predict(val_X)\nprint(mean_absolute_error(absences_predictions, val_y))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvcmodel = SVC(random_state=random_seed)\nsvcmodel.fit(train_X, train_y)\nabsences_predictions = svcmodel.predict(val_X)\nprint(mean_absolute_error(absences_predictions, val_y))\n\nscores = -1 * cross_val_score(svcmodel, train_X, train_y,\n                              cv=3,\n                            scoring='neg_mean_absolute_error')\n\nprint(scores.mean())\nprint(list(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor(random_state=random_seed, n_estimators=2000, learning_rate=0.0006)\nxgb.fit(train_X, train_y)\nabsences_predictions = xgb.predict(val_X)\nprint(mean_absolute_error(absences_predictions, val_y))\n\n\nscores = -1 * cross_val_score(xgb, train_X, train_y,\n                              cv=4,\n                            scoring='neg_mean_absolute_error')\n\nprint(scores.mean())\nprint(list(scores))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlinear = LinearRegression()\nlinear.fit(train_X, train_y)\nabsences_predictions = linear.predict(val_X)\nprint(mean_absolute_error(absences_predictions, val_y))\n\nscores = -1 * cross_val_score(linear, train_X, train_y,\n                              cv=4,\n                            scoring='neg_mean_absolute_error')\n\nprint(scores.mean())\n\nprint(list(scores))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\n\nneigh = KNeighborsRegressor()\nneigh.fit(train_X, train_y)\nabsences_predictions = neigh.predict(val_X)\nprint(mean_absolute_error(absences_predictions, val_y))\nscores = -1 * cross_val_score(neigh, train_X, train_y,\n                              cv=5,\n                            scoring='neg_mean_absolute_error')\n\nprint(scores.mean())\nprint(list(scores))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}